{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Redbrick Technical Documentation \u00b6 The idea of Redbrick documention is to keep an up to date information about the technical infrastructure of Redbrick. This is mostly intended for admins, future admins, webmasters, and everybody else who is grumpy and has no life. PS: The search box actually works... please use it for getting around - it's awesome. Quick Links \u00b6 Daily Operations Services New Admin Cheatsheet Redbrick API Useful Scripts Procedures Monitoring Hardware Network Web Postmortems NixOS , by jaysus, there's a lot New Admins \u00b6 So, you want to become an admin. Brave of you. Here's some stuff you should probably read: Becoming an admin Policies Abuse at Redbrick , and the committee's stance on it","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#redbrick-technical-documentation","text":"The idea of Redbrick documention is to keep an up to date information about the technical infrastructure of Redbrick. This is mostly intended for admins, future admins, webmasters, and everybody else who is grumpy and has no life. PS: The search box actually works... please use it for getting around - it's awesome.","title":"Redbrick Technical Documentation"},{"location":"#quick-links","text":"Daily Operations Services New Admin Cheatsheet Redbrick API Useful Scripts Procedures Monitoring Hardware Network Web Postmortems NixOS , by jaysus, there's a lot","title":"Quick Links"},{"location":"#new-admins","text":"So, you want to become an admin. Brave of you. Here's some stuff you should probably read: Becoming an admin Policies Abuse at Redbrick , and the committee's stance on it","title":"New Admins"},{"location":"api/","text":"API \u00b6 Redbrick Administrator Web API \u00b6 The Redbrick web API serves as an easy interface to carry out administrator tasks (mainly LDAP related), and for use in automation. This saves time instead of accessing machines, and formulating and executing manual LDAP queries or scripts. The server code for the API is hosted on Zeus in a docker container called 'api-redbrick', written in Python with FastAPI . Reference \u00b6 For the most up to date, rich API reference please visit https://api.redbrick.dcu.ie/docs All requests are validated with Basic Auth for access. See example. Method Route URL Parameters Body GET /users/ username username - Redbrick username N/A PUT /users/ username username - Redbrick username ldap_key POST /users/register N/A ldap_value Examples \u00b6 GET a user's LDAP data import requests url = \"https://api.redbrick.dcu.ie/users/USERNAME_HERE\" headers = { 'Authorization' : 'Basic <ENCODED_USERANDPASS_HERE>' } response = requests . request ( \"GET\" , url , headers = headers ) print ( response . text ) PUT a user's LDAP data to change their loginShell to /usr/local/shells/zsh import requests import json url = \"https://api.redbrick.dcu.ie/users/USERNAME_HERE\" payload = json . dumps ({ \"ldap_key\" : \"loginShell\" , \"ldap_value\" : \"/usr/local/shells/zsh\" }) headers = { 'Authorization' : 'Basic <ENCODED_USERANDPASS_HERE>' , 'Content-Type' : 'application/json' } response = requests . request ( \"GET\" , url , headers = headers , data = payload ) print ( response . text )","title":"API"},{"location":"api/#api","text":"","title":"API"},{"location":"api/#redbrick-administrator-web-api","text":"The Redbrick web API serves as an easy interface to carry out administrator tasks (mainly LDAP related), and for use in automation. This saves time instead of accessing machines, and formulating and executing manual LDAP queries or scripts. The server code for the API is hosted on Zeus in a docker container called 'api-redbrick', written in Python with FastAPI .","title":"Redbrick Administrator Web API"},{"location":"api/#reference","text":"For the most up to date, rich API reference please visit https://api.redbrick.dcu.ie/docs All requests are validated with Basic Auth for access. See example. Method Route URL Parameters Body GET /users/ username username - Redbrick username N/A PUT /users/ username username - Redbrick username ldap_key POST /users/register N/A ldap_value","title":"Reference"},{"location":"api/#examples","text":"GET a user's LDAP data import requests url = \"https://api.redbrick.dcu.ie/users/USERNAME_HERE\" headers = { 'Authorization' : 'Basic <ENCODED_USERANDPASS_HERE>' } response = requests . request ( \"GET\" , url , headers = headers ) print ( response . text ) PUT a user's LDAP data to change their loginShell to /usr/local/shells/zsh import requests import json url = \"https://api.redbrick.dcu.ie/users/USERNAME_HERE\" payload = json . dumps ({ \"ldap_key\" : \"loginShell\" , \"ldap_value\" : \"/usr/local/shells/zsh\" }) headers = { 'Authorization' : 'Basic <ENCODED_USERANDPASS_HERE>' , 'Content-Type' : 'application/json' } response = requests . request ( \"GET\" , url , headers = headers , data = payload ) print ( response . text )","title":"Examples"},{"location":"cheatsheet/","text":"Cheatsheet \u00b6 LDAP \u00b6 -- Query a user ldapsearch -x uid=\"USERNAME_HERE\" -- Query user as root for more detailed info ldapsearch -D \"cn=root,ou=services,o=redbrick\" -y /etc/ldap.secret uid=user -- Find all users emails created by USERNAME ldapsearch -x createdby=\"user\" uid | awk '/uid:/ {print $2\"@redbrick.dcu.ie\"}' -- Check if something is backed up on NFS ( /storage/path/to/file ) All useful LDAP scripts ( edit user quota, reset user password, renew user accounts, etc ) are located in the home directory of root on Azazel. Log in as root on a server with local accounts: ssh localaccount@redbrick.dcu.ie sudo -i # (same password as localaccount account) Authentication/Passwords \u00b6 Onboarding new admins \u00b6 Create root ssh key for NixOS Machines Following creation of the key, add to the whitelist in nix configs . ssh-keygen -t ed25519 # Generate key cat ~/.ssh/id_ed25519.pub # Verify it's been created ssh-copy-id -i ~/.ssh/id_ed25519 user@redbrick.dcu.ie # Copy to local account's ssh dir ssh -i ~/.ssh/mykey user@redbrick.dcu.ie # Verify that this key was copied Access passwordsafe (pwsafe) \u00b6 Location of master password vault. Note: getpw will prompt you for the Master root password. ssh localroot@halfpint sudo -i # to log in as root with local user password pwsafe # to list passwords getpw <name_of_pass> # Grab password by name key | getpw pygmalion SSH to root on a NixOS machine \u00b6 From the account you generated your ssh key on (in nix configs) type: ssh root@hardcase.internal NixOS \u00b6 -- Install a temporary program nix-shell -p [ space seperated package names ] -- Run brickbot2 (running on Metharme) cd brickbot2 nix-shell source venv/bin/activate python3 main.py config.toml Minecraft Servers \u00b6 The Redbrick Minecraft server's are dockerized applications running on Zeus on a server-per-container basis, using the tools on this GitHub Repo: https://github.com/itzg/docker-minecraft-server#interacting-with-the-server Repo is very well documented so have a look at the README but here's the basics: NOTE: Local Root accounts must be added to the docker group before they can run the docker commands. usermod -a -G docker ACCOUNT_NAME You can docker ps | grep minec to find the docker containers running the servers. The docker compose files are located in /etc/docker-compose/services , Unmodded Vanilla compose for example is in /etc/docker-compose/services/minecraft_unmodded/ To see the configuration for the container you can do docker inspect CONTAINER_NAME_OR_ID Interacting with the Server Console https://github.com/itzg/docker-minecraft-server#interacting-with-the-server","title":"Cheatsheet"},{"location":"cheatsheet/#cheatsheet","text":"","title":"Cheatsheet"},{"location":"cheatsheet/#ldap","text":"-- Query a user ldapsearch -x uid=\"USERNAME_HERE\" -- Query user as root for more detailed info ldapsearch -D \"cn=root,ou=services,o=redbrick\" -y /etc/ldap.secret uid=user -- Find all users emails created by USERNAME ldapsearch -x createdby=\"user\" uid | awk '/uid:/ {print $2\"@redbrick.dcu.ie\"}' -- Check if something is backed up on NFS ( /storage/path/to/file ) All useful LDAP scripts ( edit user quota, reset user password, renew user accounts, etc ) are located in the home directory of root on Azazel. Log in as root on a server with local accounts: ssh localaccount@redbrick.dcu.ie sudo -i # (same password as localaccount account)","title":"LDAP"},{"location":"cheatsheet/#authenticationpasswords","text":"","title":"Authentication/Passwords"},{"location":"cheatsheet/#onboarding-new-admins","text":"Create root ssh key for NixOS Machines Following creation of the key, add to the whitelist in nix configs . ssh-keygen -t ed25519 # Generate key cat ~/.ssh/id_ed25519.pub # Verify it's been created ssh-copy-id -i ~/.ssh/id_ed25519 user@redbrick.dcu.ie # Copy to local account's ssh dir ssh -i ~/.ssh/mykey user@redbrick.dcu.ie # Verify that this key was copied","title":"Onboarding new admins"},{"location":"cheatsheet/#access-passwordsafe-pwsafe","text":"Location of master password vault. Note: getpw will prompt you for the Master root password. ssh localroot@halfpint sudo -i # to log in as root with local user password pwsafe # to list passwords getpw <name_of_pass> # Grab password by name key | getpw pygmalion","title":"Access passwordsafe (pwsafe)"},{"location":"cheatsheet/#ssh-to-root-on-a-nixos-machine","text":"From the account you generated your ssh key on (in nix configs) type: ssh root@hardcase.internal","title":"SSH to root on a NixOS machine"},{"location":"cheatsheet/#nixos","text":"-- Install a temporary program nix-shell -p [ space seperated package names ] -- Run brickbot2 (running on Metharme) cd brickbot2 nix-shell source venv/bin/activate python3 main.py config.toml","title":"NixOS"},{"location":"cheatsheet/#minecraft-servers","text":"The Redbrick Minecraft server's are dockerized applications running on Zeus on a server-per-container basis, using the tools on this GitHub Repo: https://github.com/itzg/docker-minecraft-server#interacting-with-the-server Repo is very well documented so have a look at the README but here's the basics: NOTE: Local Root accounts must be added to the docker group before they can run the docker commands. usermod -a -G docker ACCOUNT_NAME You can docker ps | grep minec to find the docker containers running the servers. The docker compose files are located in /etc/docker-compose/services , Unmodded Vanilla compose for example is in /etc/docker-compose/services/minecraft_unmodded/ To see the configuration for the container you can do docker inspect CONTAINER_NAME_OR_ID Interacting with the Server Console https://github.com/itzg/docker-minecraft-server#interacting-with-the-server","title":"Minecraft Servers"},{"location":"hardware/","text":"Hardware \u00b6 Here is a list of current hardware in Redbrick's suite of servers, switches and other bits Servers \u00b6 Daedalus + Icarus \u00b6 These 2 PowerEdge servers are twins and thus share documentation.a Details \u00b6 Type: Dell PowerEdge 2950 Operating System: NixOS CPU: 2x Intel Xeon L5335 @ 2.00GHz RAM: 32GB (Daedalus), 16GB (Icarus) Storage: Dell Perc 6/i Integrated RAID controller Disks: 2 x 73GB SAS disks in RAID 1 (hardware) 3 x 600GB SAS disks in passthrough (3x RAID 0) Drives: Internal SATA DVD\u00b1RW Network: 2x Onboard Ethernet, 802.3ad bonding iDRAC NIC: Shared on port 1 Daedalus' IP is 0.50, Icarus' is 0.150 Their iDRAC IPs are 1.50 and 1.150 respectively Services \u00b6 LDAP NFS, (a.k.a /storage) from Icarus GlusterFS, eventually, or some other distributed storage to replace NFS Switches \u00b6 UPS \u00b6 Other \u00b6","title":"Hardware"},{"location":"hardware/#hardware","text":"Here is a list of current hardware in Redbrick's suite of servers, switches and other bits","title":"Hardware"},{"location":"hardware/#servers","text":"","title":"Servers"},{"location":"hardware/#daedalus-icarus","text":"These 2 PowerEdge servers are twins and thus share documentation.a","title":"Daedalus + Icarus"},{"location":"hardware/#details","text":"Type: Dell PowerEdge 2950 Operating System: NixOS CPU: 2x Intel Xeon L5335 @ 2.00GHz RAM: 32GB (Daedalus), 16GB (Icarus) Storage: Dell Perc 6/i Integrated RAID controller Disks: 2 x 73GB SAS disks in RAID 1 (hardware) 3 x 600GB SAS disks in passthrough (3x RAID 0) Drives: Internal SATA DVD\u00b1RW Network: 2x Onboard Ethernet, 802.3ad bonding iDRAC NIC: Shared on port 1 Daedalus' IP is 0.50, Icarus' is 0.150 Their iDRAC IPs are 1.50 and 1.150 respectively","title":"Details"},{"location":"hardware/#services","text":"LDAP NFS, (a.k.a /storage) from Icarus GlusterFS, eventually, or some other distributed storage to replace NFS","title":"Services"},{"location":"hardware/#switches","text":"","title":"Switches"},{"location":"hardware/#ups","text":"","title":"UPS"},{"location":"hardware/#other","text":"","title":"Other"},{"location":"monitoring/","text":"","title":"Monitoring"},{"location":"network/","text":"Network \u00b6 < TODO >","title":"Network"},{"location":"network/#network","text":"< TODO >","title":"Network"},{"location":"postmortems/","text":"Postmortems \u00b6 Any postmortems after incidents will be posted here.","title":"Postmortems"},{"location":"postmortems/#postmortems","text":"Any postmortems after incidents will be posted here.","title":"Postmortems"},{"location":"procedures/","text":"Procedures \u00b6 New elected admins \u00b6 The chronological process of becoming an admin usually looks very similar each year. There are some important things you should know. Remember, being a SysAdmin for the society is not a job, it is a volunteered task you sign up to - don't stress yourself out over it, have fun, and hopefully learn a thing or two. : ) Process \u00b6 Admin Exam \u00b6 Anyone wishing to run and be elected as a SysAdmin must complete a technical exam as an assessment of your knowledge and competency in solving some of the many problems that will be thrown at you. You can find some archives of past exams here , however note that these vary year to year as they are created each year by the currently elected admins. Election at AGM \u00b6 At the annual general meeting, you may nominate yourself, or have someone nominate you to run for SysAdmin. You may only run if you have passed the Admin exam. The amount of admins per year is usually three, to be elected, you must be in the top three voted members. Onboarding \u00b6 If you are successfully elected - congrats! We welcome you to this pain joy filled journey :) After being elected it is your time to learn the ropes and become familiar with the technicalities of Redbrick. Not alone of course! The previous Admins will assist you on this journey and be there to answer any of your questions, along with this documentation. Post-powercut Todo List \u00b6 A list of things that should be done/checked immediately after a power cut: Check KVM, hit ctrl+D on minerva to make sure it boots. Check KVM, hit F1 on sprout to make sure it boots Check KVM, sometimes you need to press F1 on carbon for it to boot Stop Exim on the mail server (Morpheus) until minerva (NFS) is online. If LDAP is down, you'll need to use the ALOM to do the next step. Check that ldapclient started (svcs -xv). If it didn't, run svcadm clear ldap/client to make it start. This usually happens because murphy comes back before morpheus does, and the LDAP client won't start due to lack of an LDAP server. Apache is stupid and tries to start before networking is finished starting. To fix it, disable/re-enable it a few times. This usually makes it turn on. NixOS \u00b6 Familiarise yourself with the layout of the following. Bookmarking the page is also a good shout. NixOS documentation Who is NixOS and what does he do \u00b6 NixOS is a distribution of linux that is focused on having a config-first operating system to run services. The advantages of such an approach are the following: Files dictate how an installation is set up, and as such, can be versoined and tracked in your favorite VCS. New configs can be tested, and safely rolled back. Can be used for both physical and virtual machines in the same way. Further reading on this can be found on the about page . Being an admin: NixOS and you \u00b6 There's a couple of things you'll need to do before you get started with NixOS First and foremost is to get set up to contribute to the Redbrick nix-configs repo . Depending on the powers that be, some sort of normal pr contribution will be acceptable, if you have access a branch is appropriate, in all other cases make a fork and pr back to Redbrick's repo. This will be case by case for those of you reading. Here's a quick hit list of stuff that's worthy of book marking also as you work with Nix: NixOS Wiki NixOS Manual Nixpkgs index (unstable means changing, not buggy) Grafana config options (as an example of how to configure an individual service) Nix is pretty small as an OS so setting yourself up a node, either as a home server, or as a VM is a solid way to practice how stuff works in an actual environment and lets you work independantly of Redbrick. A service you configure at home should be able to run on Redbrick, and vice versa. Getting set up to start deploying stuff \u00b6 The first step is to navigate to the ssh service config in the nix-config repo here . Make a pull request asking to add the PUBLIC KEY of your ssh key pait to the config file. The best thing to do is to copy the previous line and modify it to contain your details instead. At time of writing, it is expected for you to generate a ssh-ed25519 key. This is subject to change with new cryprographic standards. Once this is done, contact one of the currently set up users to pull and reload the given machines and you'll have access right away using the accompanying key. IRC Ops \u00b6 This is a mirror of: Redbrick cmt Wiki entry Channel Modes \u00b6 It's easy to bugger up the channel with the MODE command, so here's a nice copied and pasted summary of how to use it: /mode {channel} +b {nick|address} - ban somebody by nickname or address mask (nick!account@host) /mode {channel} +i - channel is invite-only /mode {channel} +l {number} - channel is limited, with {number} users allowed maximal /mode {channel} +m - channel is moderated, only chanops and others with 'voice' can talk/mode {channel} +n external /MSG s to channel are not allowed. /mode {channel} +p - channel is private /mode {channel} +s - channel is secret /mode {channel} +t topic - limited, only chanops may change it /mode {channel} +o {nick} - makes {nick} a channel operator /mode {channel} +v {nick} - gives {nick} a voice Other Commands \u00b6 Basically what you'll be using is: To kick someone: /kick username To ban someone: /mode #lobby +b username To set the topic: /topic #lobby whatever To op someone: /mode #lobby +o someone To op two people: /mode #lobby +oo someone someone_else Or: To kick someone: /k username To ban someone: /ban username To unban someone: /unban username To set the topic: /t whatever To op someone: /op someone To op two people: /op someone someone_else To deop someone: /deop someone Sysop specific commands \u00b6 These commands can only be run by sysops (i.e. admins in the ircd config file). Enter BOFH mode (required for all sysop commands): /oper Peer to another server*: /sconnect <node name> Drop a peer with another server: /squit <node name> Force op yourself ( do not abuse ): /quote opme <channel name> Barge into a channel uninvited ( again, do not abuse ): /quote ojoin #channel Barge into a channel uninvited with ops ( same again ): /quote ojoin @#channel Force someone to join a channel: /quote forcejoin nick #channel Kill someone: /kill <username> <smartassed kill messsage> Ban someone from this server: /kline <username> (there may be more params on this) Ban someone from the entire network: /gline <username> (there may be more params on this) (thanks to atlas for the quick overview) Don't try connect to intersocs. Due to crazy endian issues or something they have to connect to us. Bots \u00b6 It has now become a slight problem with so many bots 'littering' #lobby that anyone wishing to add a new bot to the channel must request permission from the Committee. The main feature wanted is a time limit on bot commands. Services \u00b6 The IRC services run by Trinity for all the netsocs. The two services are NickServ and ChanServ . /msg NickServ HELP /msg ChanServ HELP for more details. Redbrick System Administrator Policies \u00b6 The purpose of this is to brief new Redbrick system administrators on the current setup, policies and practices in place and to serve as the place to record all such information for current and future administrators. Admin Account Priviliges \u00b6 By default, all admin accounts will remain the same as the rest of the committee. Each admin will recieve a local account on each machine that will be in the root group. This allows you to log on if ldap goes down. Accounts should not be placed into any other 'system' or priviliged accounts (e.g. pgsql, mail, news, etc.) but by all accounts (hah, bad pun!) can be placed into useful groups (e.g. cvs, webgroup, helpdesk etc.) Root account \u00b6 When su'ing to root, please observe the following: Wait for the password prompt before typing in the password! Sometimes lag/terminal freezes or whatever can kick in. The other classic mistake is typing the password in place of the username (say for a console login). Make sure LOGNAME is set to your unix name. The linux boxes will prompt you for this. On OpenBSD you can use 'su -m' to keep the environment. Don't change the root account/finger information! If you wish to use another shell, place customisations in your own file. For bash, /root/.bash_profile.<USERNAME> and for zsh /root/.zshrc.<USERNAME> . /root/.zshrc and /root/.bash_profile source in the appropriate file as long as $LOGNAME is set right (see above). Do not put personal customisations into the default root account setup, remember other people have to use it. Common aliases can be put in /root/.profile, familiarise yourself with the existing ones, they can come in handy. Please keep /root tidy. Don't leave stuff strewn about the place! Make sure to check permissions and ownership on files you work on constantly especially files with important or sensitive information in them (e.g. always use cp -p when copying stuff about). Only use root account when absolutely necessary. Many admin tasks can be done or tested first as a regular user. Gotchas \u00b6 Couple of things to look out for: killall command, never ever use it! Alias cp , mv & rm with the -i option. If you're ever unsure, don't! Ask another admin or check the docs. Always always double check commands before firing them off! Admin Mailing Lists \u00b6 lists.redbrick.dcu.ie (Postorius) All accounts in the root group must be on the admin mailing list and vice versa. Admins who leave/join the root group must be added/removed from the list respectively. Elected Admins should also be on the elected-admins list. This address is mainly used for mail to PayPal, user renewals, registration, and general administration tasks. It is the responsibility of the Elected Admins to ensure that all mailing lists (committee, helpdesk, webmaster, elected-admins, admins, etc) are all up-to-date. Admin Account Responsibilities \u00b6 As an adminisitrator, your new local account has extra priviliges (namely being in the root group). For this reason, you should not run any untrusted or unknown programs or scripts. If you must, and source code is available you should check it before running it. Compile your own versions of other user's programs you use regularly. It is far too easy for other users to trojan your account in this manner and get root. Do not use passwordless ssh keys on any of your accounts. When using an untrusted workstation (i.e. just about any PC in DCU!) always check for keyloggers running on the local machine and never use any non system or non personal copies of PuTTY/ssh - there's no way of knowing if they have been trojaned. General Responsibilities \u00b6 Look after and regularly monitor all systems, network, hardware and user requests (ones that fall outside of helpdesk's realm, of course!). Actively ensure system and network security. We can't police all user accounts and activities, but basic system security is paramount! Keep uptodate with bugtraq/securityfocus etc. Check system logs regularly, process listings, network connections, disk usage, etc. Downtime \u00b6 All downtime must be scheduled and notified to the members well in advance by means of motd & announce . If it's really important, a mail to announce-redbrick and socials post may be necessary. All unexpected/unscheduled downtime (as a result of a crash or as an emergency precaution) must be explained to the members as soon as possible after the system is brought back. A post to announce, notice in motd or possibly a mail to committee/admins is sufficient. When performing a shutdown, start the shutdown 5 or 10 minutes in advance of the scheduled shutdown time to give people a chance to logout. It may also be useful to disable logins at this stage with a quick explanation in /etc/nologin . Documentation \u00b6 Please read all the documentation before you do anything, but remember that the docs aren't complete and are sometimes out of date. Please update them as you go :D","title":"Procedures"},{"location":"procedures/#procedures","text":"","title":"Procedures"},{"location":"procedures/#new-elected-admins","text":"The chronological process of becoming an admin usually looks very similar each year. There are some important things you should know. Remember, being a SysAdmin for the society is not a job, it is a volunteered task you sign up to - don't stress yourself out over it, have fun, and hopefully learn a thing or two. : )","title":"New elected admins"},{"location":"procedures/#process","text":"","title":"Process"},{"location":"procedures/#admin-exam","text":"Anyone wishing to run and be elected as a SysAdmin must complete a technical exam as an assessment of your knowledge and competency in solving some of the many problems that will be thrown at you. You can find some archives of past exams here , however note that these vary year to year as they are created each year by the currently elected admins.","title":"Admin Exam"},{"location":"procedures/#election-at-agm","text":"At the annual general meeting, you may nominate yourself, or have someone nominate you to run for SysAdmin. You may only run if you have passed the Admin exam. The amount of admins per year is usually three, to be elected, you must be in the top three voted members.","title":"Election at AGM"},{"location":"procedures/#onboarding","text":"If you are successfully elected - congrats! We welcome you to this pain joy filled journey :) After being elected it is your time to learn the ropes and become familiar with the technicalities of Redbrick. Not alone of course! The previous Admins will assist you on this journey and be there to answer any of your questions, along with this documentation.","title":"Onboarding"},{"location":"procedures/#post-powercut-todo-list","text":"A list of things that should be done/checked immediately after a power cut: Check KVM, hit ctrl+D on minerva to make sure it boots. Check KVM, hit F1 on sprout to make sure it boots Check KVM, sometimes you need to press F1 on carbon for it to boot Stop Exim on the mail server (Morpheus) until minerva (NFS) is online. If LDAP is down, you'll need to use the ALOM to do the next step. Check that ldapclient started (svcs -xv). If it didn't, run svcadm clear ldap/client to make it start. This usually happens because murphy comes back before morpheus does, and the LDAP client won't start due to lack of an LDAP server. Apache is stupid and tries to start before networking is finished starting. To fix it, disable/re-enable it a few times. This usually makes it turn on.","title":"Post-powercut Todo List"},{"location":"procedures/#nixos","text":"Familiarise yourself with the layout of the following. Bookmarking the page is also a good shout. NixOS documentation","title":"NixOS"},{"location":"procedures/#who-is-nixos-and-what-does-he-do","text":"NixOS is a distribution of linux that is focused on having a config-first operating system to run services. The advantages of such an approach are the following: Files dictate how an installation is set up, and as such, can be versoined and tracked in your favorite VCS. New configs can be tested, and safely rolled back. Can be used for both physical and virtual machines in the same way. Further reading on this can be found on the about page .","title":"Who is NixOS and what does he do"},{"location":"procedures/#being-an-admin-nixos-and-you","text":"There's a couple of things you'll need to do before you get started with NixOS First and foremost is to get set up to contribute to the Redbrick nix-configs repo . Depending on the powers that be, some sort of normal pr contribution will be acceptable, if you have access a branch is appropriate, in all other cases make a fork and pr back to Redbrick's repo. This will be case by case for those of you reading. Here's a quick hit list of stuff that's worthy of book marking also as you work with Nix: NixOS Wiki NixOS Manual Nixpkgs index (unstable means changing, not buggy) Grafana config options (as an example of how to configure an individual service) Nix is pretty small as an OS so setting yourself up a node, either as a home server, or as a VM is a solid way to practice how stuff works in an actual environment and lets you work independantly of Redbrick. A service you configure at home should be able to run on Redbrick, and vice versa.","title":"Being an admin: NixOS and you"},{"location":"procedures/#getting-set-up-to-start-deploying-stuff","text":"The first step is to navigate to the ssh service config in the nix-config repo here . Make a pull request asking to add the PUBLIC KEY of your ssh key pait to the config file. The best thing to do is to copy the previous line and modify it to contain your details instead. At time of writing, it is expected for you to generate a ssh-ed25519 key. This is subject to change with new cryprographic standards. Once this is done, contact one of the currently set up users to pull and reload the given machines and you'll have access right away using the accompanying key.","title":"Getting set up to start deploying stuff"},{"location":"procedures/#irc-ops","text":"This is a mirror of: Redbrick cmt Wiki entry","title":"IRC Ops"},{"location":"procedures/#channel-modes","text":"It's easy to bugger up the channel with the MODE command, so here's a nice copied and pasted summary of how to use it: /mode {channel} +b {nick|address} - ban somebody by nickname or address mask (nick!account@host) /mode {channel} +i - channel is invite-only /mode {channel} +l {number} - channel is limited, with {number} users allowed maximal /mode {channel} +m - channel is moderated, only chanops and others with 'voice' can talk/mode {channel} +n external /MSG s to channel are not allowed. /mode {channel} +p - channel is private /mode {channel} +s - channel is secret /mode {channel} +t topic - limited, only chanops may change it /mode {channel} +o {nick} - makes {nick} a channel operator /mode {channel} +v {nick} - gives {nick} a voice","title":"Channel Modes"},{"location":"procedures/#other-commands","text":"Basically what you'll be using is: To kick someone: /kick username To ban someone: /mode #lobby +b username To set the topic: /topic #lobby whatever To op someone: /mode #lobby +o someone To op two people: /mode #lobby +oo someone someone_else Or: To kick someone: /k username To ban someone: /ban username To unban someone: /unban username To set the topic: /t whatever To op someone: /op someone To op two people: /op someone someone_else To deop someone: /deop someone","title":"Other Commands"},{"location":"procedures/#sysop-specific-commands","text":"These commands can only be run by sysops (i.e. admins in the ircd config file). Enter BOFH mode (required for all sysop commands): /oper Peer to another server*: /sconnect <node name> Drop a peer with another server: /squit <node name> Force op yourself ( do not abuse ): /quote opme <channel name> Barge into a channel uninvited ( again, do not abuse ): /quote ojoin #channel Barge into a channel uninvited with ops ( same again ): /quote ojoin @#channel Force someone to join a channel: /quote forcejoin nick #channel Kill someone: /kill <username> <smartassed kill messsage> Ban someone from this server: /kline <username> (there may be more params on this) Ban someone from the entire network: /gline <username> (there may be more params on this) (thanks to atlas for the quick overview) Don't try connect to intersocs. Due to crazy endian issues or something they have to connect to us.","title":"Sysop specific commands"},{"location":"procedures/#bots","text":"It has now become a slight problem with so many bots 'littering' #lobby that anyone wishing to add a new bot to the channel must request permission from the Committee. The main feature wanted is a time limit on bot commands.","title":"Bots"},{"location":"procedures/#services","text":"The IRC services run by Trinity for all the netsocs. The two services are NickServ and ChanServ . /msg NickServ HELP /msg ChanServ HELP for more details.","title":"Services"},{"location":"procedures/#redbrick-system-administrator-policies","text":"The purpose of this is to brief new Redbrick system administrators on the current setup, policies and practices in place and to serve as the place to record all such information for current and future administrators.","title":"Redbrick System Administrator Policies"},{"location":"procedures/#admin-account-priviliges","text":"By default, all admin accounts will remain the same as the rest of the committee. Each admin will recieve a local account on each machine that will be in the root group. This allows you to log on if ldap goes down. Accounts should not be placed into any other 'system' or priviliged accounts (e.g. pgsql, mail, news, etc.) but by all accounts (hah, bad pun!) can be placed into useful groups (e.g. cvs, webgroup, helpdesk etc.)","title":"Admin Account Priviliges"},{"location":"procedures/#root-account","text":"When su'ing to root, please observe the following: Wait for the password prompt before typing in the password! Sometimes lag/terminal freezes or whatever can kick in. The other classic mistake is typing the password in place of the username (say for a console login). Make sure LOGNAME is set to your unix name. The linux boxes will prompt you for this. On OpenBSD you can use 'su -m' to keep the environment. Don't change the root account/finger information! If you wish to use another shell, place customisations in your own file. For bash, /root/.bash_profile.<USERNAME> and for zsh /root/.zshrc.<USERNAME> . /root/.zshrc and /root/.bash_profile source in the appropriate file as long as $LOGNAME is set right (see above). Do not put personal customisations into the default root account setup, remember other people have to use it. Common aliases can be put in /root/.profile, familiarise yourself with the existing ones, they can come in handy. Please keep /root tidy. Don't leave stuff strewn about the place! Make sure to check permissions and ownership on files you work on constantly especially files with important or sensitive information in them (e.g. always use cp -p when copying stuff about). Only use root account when absolutely necessary. Many admin tasks can be done or tested first as a regular user.","title":"Root account"},{"location":"procedures/#gotchas","text":"Couple of things to look out for: killall command, never ever use it! Alias cp , mv & rm with the -i option. If you're ever unsure, don't! Ask another admin or check the docs. Always always double check commands before firing them off!","title":"Gotchas"},{"location":"procedures/#admin-mailing-lists","text":"lists.redbrick.dcu.ie (Postorius) All accounts in the root group must be on the admin mailing list and vice versa. Admins who leave/join the root group must be added/removed from the list respectively. Elected Admins should also be on the elected-admins list. This address is mainly used for mail to PayPal, user renewals, registration, and general administration tasks. It is the responsibility of the Elected Admins to ensure that all mailing lists (committee, helpdesk, webmaster, elected-admins, admins, etc) are all up-to-date.","title":"Admin Mailing Lists"},{"location":"procedures/#admin-account-responsibilities","text":"As an adminisitrator, your new local account has extra priviliges (namely being in the root group). For this reason, you should not run any untrusted or unknown programs or scripts. If you must, and source code is available you should check it before running it. Compile your own versions of other user's programs you use regularly. It is far too easy for other users to trojan your account in this manner and get root. Do not use passwordless ssh keys on any of your accounts. When using an untrusted workstation (i.e. just about any PC in DCU!) always check for keyloggers running on the local machine and never use any non system or non personal copies of PuTTY/ssh - there's no way of knowing if they have been trojaned.","title":"Admin Account Responsibilities"},{"location":"procedures/#general-responsibilities","text":"Look after and regularly monitor all systems, network, hardware and user requests (ones that fall outside of helpdesk's realm, of course!). Actively ensure system and network security. We can't police all user accounts and activities, but basic system security is paramount! Keep uptodate with bugtraq/securityfocus etc. Check system logs regularly, process listings, network connections, disk usage, etc.","title":"General Responsibilities"},{"location":"procedures/#downtime","text":"All downtime must be scheduled and notified to the members well in advance by means of motd & announce . If it's really important, a mail to announce-redbrick and socials post may be necessary. All unexpected/unscheduled downtime (as a result of a crash or as an emergency precaution) must be explained to the members as soon as possible after the system is brought back. A post to announce, notice in motd or possibly a mail to committee/admins is sufficient. When performing a shutdown, start the shutdown 5 or 10 minutes in advance of the scheduled shutdown time to give people a chance to logout. It may also be useful to disable logins at this stage with a quick explanation in /etc/nologin .","title":"Downtime"},{"location":"procedures/#documentation","text":"Please read all the documentation before you do anything, but remember that the docs aren't complete and are sometimes out of date. Please update them as you go :D","title":"Documentation"},{"location":"roadmap/","text":"Roadmap \u00b6 Operating Systems Of Choice \u00b6 NixOS -> Used on Motherlode Ubuntu -> Login boxes Debian -> Used for all other machines Why? Sensible defaults (/etc/resolve.conf etc.) No snap Debian is as close to the most popular distribution as possible Important (Core) Services \u00b6 In order of priority. DNS Migrate to Fred Set up on new server Clean up the zone file /storage/ NFS, backups, database (from Icarus) and failover (from Daedalus) LDAP (Daedalus, Icarus read-only slave) pwsafe (look at changing to hashcorp vault/bitwarden) -> Login Machines (Ubuntu) \u00b6 Azazel Pygmalion Zeus (Non-login -> Designated Docker Host) -> Multipurpose Machines (Debian) \u00b6 Halfpint Paphos Daedalus Icarus Albus Clyde Hardcase Metharme Fred -> Designated Nix Machine \u00b6 Motherlode. Why? Clubs and Socs and other services like Mail are quite honestly easiest done using Nix configs (even though it can be disgusting). It is a viable choice to solve a hard problem. Services Using Nix to be moved to Motherlode \u00b6 Mailman Certs Grafana (X) Httpd (Apache) ircd LDAP Postfix zfsquota bitlbee git glusterfs libvrt Loki (X) postgres prometheus (X) promtail rbbackup redis squid sshnix (ssh keys) thelounge (X) znapsend Docs \u00b6 Update fucking.readthedocs.io to new home, docs.redbrick.dcu.ie TODO \u00b6 In order of priority. Update docs Material for MkDocs Make passwordsafe redundant Migrate to BitWarden Use Fred as a temporary host while we decide what to make it's permanent host Azazel needs to be functional Authentication missing local users Fred becomes new DNS host Remove DNS from Paphos Databases moved to Master/Redundancy Machines Databases are dependancies for the services to come Hardcase and Metharme Move all services to remain to Motherlode/Nix Remove redundant services (marked X) Firewall cleanup Assign IP's Stable Monitoring setup for all servers and services","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#operating-systems-of-choice","text":"NixOS -> Used on Motherlode Ubuntu -> Login boxes Debian -> Used for all other machines Why? Sensible defaults (/etc/resolve.conf etc.) No snap Debian is as close to the most popular distribution as possible","title":"Operating Systems Of Choice"},{"location":"roadmap/#important-core-services","text":"In order of priority. DNS Migrate to Fred Set up on new server Clean up the zone file /storage/ NFS, backups, database (from Icarus) and failover (from Daedalus) LDAP (Daedalus, Icarus read-only slave) pwsafe (look at changing to hashcorp vault/bitwarden)","title":"Important (Core) Services"},{"location":"roadmap/#-login-machines-ubuntu","text":"Azazel Pygmalion Zeus (Non-login -> Designated Docker Host)","title":"-&gt; Login Machines (Ubuntu)"},{"location":"roadmap/#-multipurpose-machines-debian","text":"Halfpint Paphos Daedalus Icarus Albus Clyde Hardcase Metharme Fred","title":"-&gt; Multipurpose Machines (Debian)"},{"location":"roadmap/#-designated-nix-machine","text":"Motherlode. Why? Clubs and Socs and other services like Mail are quite honestly easiest done using Nix configs (even though it can be disgusting). It is a viable choice to solve a hard problem.","title":"-&gt; Designated Nix Machine"},{"location":"roadmap/#services-using-nix-to-be-moved-to-motherlode","text":"Mailman Certs Grafana (X) Httpd (Apache) ircd LDAP Postfix zfsquota bitlbee git glusterfs libvrt Loki (X) postgres prometheus (X) promtail rbbackup redis squid sshnix (ssh keys) thelounge (X) znapsend","title":"Services Using Nix to be moved to Motherlode"},{"location":"roadmap/#docs","text":"Update fucking.readthedocs.io to new home, docs.redbrick.dcu.ie","title":"Docs"},{"location":"roadmap/#todo","text":"In order of priority. Update docs Material for MkDocs Make passwordsafe redundant Migrate to BitWarden Use Fred as a temporary host while we decide what to make it's permanent host Azazel needs to be functional Authentication missing local users Fred becomes new DNS host Remove DNS from Paphos Databases moved to Master/Redundancy Machines Databases are dependancies for the services to come Hardcase and Metharme Move all services to remain to Motherlode/Nix Remove redundant services (marked X) Firewall cleanup Assign IP's Stable Monitoring setup for all servers and services","title":"TODO"},{"location":"scripts/","text":"","title":"Scripts"},{"location":"web/","text":"","title":"Web"},{"location":"services/","text":"Preface \u00b6 Here you will find a list of all the services Redbrick runs, along with some configs and some important information surrounding them. Adding More Services \u00b6 In order to add a new service, you will need to edit the docs repository. Adding a new service is as easy as creating a new file in docs/services/ with an appropriate name, and adding the reference to mkdocs.yml in the root of the repository. The style guide for a service file should be as follows: # ServiceName - `username` Short description on how the service works and where it is running ## Configuration Add some possible useful configs here, like a docker-compose file, certain command you may have had to run, or something that is not very obvious. Look at other services for hints on this.","title":"Preface"},{"location":"services/#preface","text":"Here you will find a list of all the services Redbrick runs, along with some configs and some important information surrounding them.","title":"Preface"},{"location":"services/#adding-more-services","text":"In order to add a new service, you will need to edit the docs repository. Adding a new service is as easy as creating a new file in docs/services/ with an appropriate name, and adding the reference to mkdocs.yml in the root of the repository. The style guide for a service file should be as follows: # ServiceName - `username` Short description on how the service works and where it is running ## Configuration Add some possible useful configs here, like a docker-compose file, certain command you may have had to run, or something that is not very obvious. Look at other services for hints on this.","title":"Adding More Services"},{"location":"services/bind/","text":"Bind9 - distro , ylmcc \u00b6 Bind9 is our DNS provider. Currently it runs on Paphos, but is being moved to Fred during the restructuring. Configuration \u00b6 The config files for bind are located in /etc/bind/master/ . The most important file in this directory is the db.Redbrick.dcu.ie file. You must never update this file without following the steps below first! Updating DNS \u00b6 To update DNS: Change directory to /etc/bind/master Back up the db.Redbrick.dcu.ie file, usually to db.Redbrick.dcu.ie.bak Run rndc freeze redbrick.dcu.ie - this stops changes to the file affecting dns while you edit it Edit db.Redbrick.dcu.ie Before changing any DNS entry in the file, you must edit the serial number on 4. You can increment it by one if you want, or follow the format: YYYYMMDDrev where rev is revision Once you are happy with your file, you can check it with named-checkzone redbrick.dcu.ie db.Redbrick.dcu.ie If this returns no errors, you are free to run rndc thaw redbrick.dcu.ie Check the status of bind9 by running service bind9 status You can access more logs from bind9 by checking /var/log/named/default.log .","title":"Bind9"},{"location":"services/bind/#bind9-distro-ylmcc","text":"Bind9 is our DNS provider. Currently it runs on Paphos, but is being moved to Fred during the restructuring.","title":"Bind9 - distro, ylmcc"},{"location":"services/bind/#configuration","text":"The config files for bind are located in /etc/bind/master/ . The most important file in this directory is the db.Redbrick.dcu.ie file. You must never update this file without following the steps below first!","title":"Configuration"},{"location":"services/bind/#updating-dns","text":"To update DNS: Change directory to /etc/bind/master Back up the db.Redbrick.dcu.ie file, usually to db.Redbrick.dcu.ie.bak Run rndc freeze redbrick.dcu.ie - this stops changes to the file affecting dns while you edit it Edit db.Redbrick.dcu.ie Before changing any DNS entry in the file, you must edit the serial number on 4. You can increment it by one if you want, or follow the format: YYYYMMDDrev where rev is revision Once you are happy with your file, you can check it with named-checkzone redbrick.dcu.ie db.Redbrick.dcu.ie If this returns no errors, you are free to run rndc thaw redbrick.dcu.ie Check the status of bind9 by running service bind9 status You can access more logs from bind9 by checking /var/log/named/default.log .","title":"Updating DNS"},{"location":"services/codimd/","text":"CodiMD - distro \u00b6 CodiMD lives on Zeus as a docker container. It is accessible through md.redbrick.dcu.ie . CodiMD is built locally and is based on codimd , the docs for which are here . Hackmd auths against ldap and its configuration is controlled from docker-compose. Go to /etc/docker-compose/services/hackmd on zeus to find the configuration. See CodiMD github for more info on configuration. The important points are disabling anonymus users and the ldap settings.","title":"CodiMD"},{"location":"services/codimd/#codimd-distro","text":"CodiMD lives on Zeus as a docker container. It is accessible through md.redbrick.dcu.ie . CodiMD is built locally and is based on codimd , the docs for which are here . Hackmd auths against ldap and its configuration is controlled from docker-compose. Go to /etc/docker-compose/services/hackmd on zeus to find the configuration. See CodiMD github for more info on configuration. The important points are disabling anonymus users and the ldap settings.","title":"CodiMD - distro"},{"location":"services/gitea/","text":"Gitea \u00b6 Redbrick uses Gitea as an open source git host. Gitea docs Gogs docs , not really important, but Gitea is built on Gogs Link to Redbrick deployment Deployment \u00b6 Gitea and its database are deployed to Hardcase which runs NixOS The actual repositories are stored in /zroot/git and most other data is stored in /var/lib/gitea The SECRET_KEY and INTERNAL_TOKEN_URI are stored in /var/secrets . They are not automatically created and must be copied when setting up new hosts. Permissions on the gitea_token.secret must be 740 and owned by git:gitea Make sure that the gitea_token.secret does NOT have a newline character in it. Other Notes \u00b6 The Giteadmin credentials are in the passwordsafe. Operation \u00b6 Gitea is very well documented in itself. Here's a couple of special commands when deploying/migrating Gitea to a different host. # Regenerate hooks which fixes push errors /path/to/gitea admin regenerate hooks # If you didn't copy the authorized_keys folder then regen that too /path/to/gitea admin regenerate keys","title":"Gitea"},{"location":"services/gitea/#gitea","text":"Redbrick uses Gitea as an open source git host. Gitea docs Gogs docs , not really important, but Gitea is built on Gogs Link to Redbrick deployment","title":"Gitea"},{"location":"services/gitea/#deployment","text":"Gitea and its database are deployed to Hardcase which runs NixOS The actual repositories are stored in /zroot/git and most other data is stored in /var/lib/gitea The SECRET_KEY and INTERNAL_TOKEN_URI are stored in /var/secrets . They are not automatically created and must be copied when setting up new hosts. Permissions on the gitea_token.secret must be 740 and owned by git:gitea Make sure that the gitea_token.secret does NOT have a newline character in it.","title":"Deployment"},{"location":"services/gitea/#other-notes","text":"The Giteadmin credentials are in the passwordsafe.","title":"Other Notes"},{"location":"services/gitea/#operation","text":"Gitea is very well documented in itself. Here's a couple of special commands when deploying/migrating Gitea to a different host. # Regenerate hooks which fixes push errors /path/to/gitea admin regenerate hooks # If you didn't copy the authorized_keys folder then regen that too /path/to/gitea admin regenerate keys","title":"Operation"},{"location":"services/icecast/","text":"Icecast - mcmahon \u00b6 Icecast is a streaming server that we currently host on Paphos. We stream DCUFm's Broadcasts to their apps via a stream presented on dcufm.redbrick.dcu.ie:80 . They serve an audio stream (stream128.mp3) via butt on a desktop in their studio to icecast2 . Icecast requires root privilege to bind to Port 80; normally icecast2 runs as the icecast2 user and binds to 8001 . Procedure \u00b6 The configuration file for icecast is located at /etc/icecast2/icecast.xml . <!-- Sources log in with username 'source' --> <-- This is the audio source. <source-password> $password1 </source-password> <-- This must be copied for the DCUFM buttrc. <!-- Relays log in username 'relay' --> <relay-password> $password2 </relay-password> <admin-user>admin</admin-user> <-- This is for the WebUI frontend <admin-password> $password3 </admin-password> <hostname>dcufm.redbrick.dcu.ie</hostname> <listen-socket> <port>80</port> <bind-address>136.206.15.101</bind-address> <-- i.p. addr for dcufm.redbrick.dcu.ie A Record. After that you must configure the default behaviour for the icecast server to allow icecast2 to bind to port 80. Set USERID & GROUPID in /etc/defaults/icecast2 to root .","title":"Icecast"},{"location":"services/icecast/#icecast-mcmahon","text":"Icecast is a streaming server that we currently host on Paphos. We stream DCUFm's Broadcasts to their apps via a stream presented on dcufm.redbrick.dcu.ie:80 . They serve an audio stream (stream128.mp3) via butt on a desktop in their studio to icecast2 . Icecast requires root privilege to bind to Port 80; normally icecast2 runs as the icecast2 user and binds to 8001 .","title":"Icecast - mcmahon"},{"location":"services/icecast/#procedure","text":"The configuration file for icecast is located at /etc/icecast2/icecast.xml . <!-- Sources log in with username 'source' --> <-- This is the audio source. <source-password> $password1 </source-password> <-- This must be copied for the DCUFM buttrc. <!-- Relays log in username 'relay' --> <relay-password> $password2 </relay-password> <admin-user>admin</admin-user> <-- This is for the WebUI frontend <admin-password> $password3 </admin-password> <hostname>dcufm.redbrick.dcu.ie</hostname> <listen-socket> <port>80</port> <bind-address>136.206.15.101</bind-address> <-- i.p. addr for dcufm.redbrick.dcu.ie A Record. After that you must configure the default behaviour for the icecast server to allow icecast2 to bind to port 80. Set USERID & GROUPID in /etc/defaults/icecast2 to root .","title":"Procedure"},{"location":"services/irc/","text":"IRC \u00b6 Redbrick InspIRCd \u00b6 In 2016/2017 we began work to move to InspIRCd. This was due to the complications in ircd-hybrid and how old it was. These complications stopped new netsocs joining us so we all agreed to move irc. $ 4 years later after multiple attempts we had not migrated. Until TCD decided to shutdown their server breaking the network. We run Inspircd v3 on Metharme. InspIRCd's docs can be found here for configuration specifics. IRC is available at irc.redbrick.dcu.ie on port 6697 . SSL is required for connection, we do not support non-SSL. When connecting from a redbrick server a user will be automatically logged in. If connecting from an external server a user must pass their password on login. For the purpose of external peering of other servers the port 7001 is expose as well. Similarly to clients we only support SSL on this port. For docs on connecting and using an IRC client please refer to the wiki . Installation \u00b6 InspIRCd is installed with Nix. There is no Nix package for InspIRCd so we compile a specific git tag from source. See Nix package for details on how it is compiled. Given we only support SSL and require LDAP, we need to enable both at compile time. Configuration \u00b6 InspIRCd's configuration is in Nix here . This config will be converted to xml on disc. Important Configuration \u00b6 oper is a list of admin users on the irc server. Their OPER password will need to be manually hashed with hmac-sha256 , and placed in a secret on the server to be read in by inspircd. ldapwhitelist is a list of cidr addresses that do no require authentication. The list consists of Redbrick public and private addresses as well as oldsoc . link is a list of all servers we peer with including the anope services server that runs on the same box. oldsoc.net \u00b6 oldsoc.net is a server run by old TCD netsocers. All the users on it are the remaining TCD associates following the shutdown of TCD IRCd. This server is maintained by its own users and has explicit permission to join IRC without LDAP auth. Anope \u00b6 Redbrick runs Anope services for the entire network. As with inspircd we compile from source. Refer to anopes github docs for configuration specifics. Our current Anope is configured with standard mods of chanserv, nickserv and operserv. All config is in here . Anope stores all info in a custom db file on disk. Discord Bridge - butlerx \u00b6 We run a bridge between the Redbrick Discord and irc. The configuration for this is here . The bridge adds all users from discord with the suffix _d2 and all irc users appear as them self but tagged as a bot in discord. Not all discord channels are on IRC, the config above contains a mapping of irc channels to discord channels id's. This needs to be manually updated to add more channels.","title":"IRC"},{"location":"services/irc/#irc","text":"","title":"IRC"},{"location":"services/irc/#redbrick-inspircd","text":"In 2016/2017 we began work to move to InspIRCd. This was due to the complications in ircd-hybrid and how old it was. These complications stopped new netsocs joining us so we all agreed to move irc. $ 4 years later after multiple attempts we had not migrated. Until TCD decided to shutdown their server breaking the network. We run Inspircd v3 on Metharme. InspIRCd's docs can be found here for configuration specifics. IRC is available at irc.redbrick.dcu.ie on port 6697 . SSL is required for connection, we do not support non-SSL. When connecting from a redbrick server a user will be automatically logged in. If connecting from an external server a user must pass their password on login. For the purpose of external peering of other servers the port 7001 is expose as well. Similarly to clients we only support SSL on this port. For docs on connecting and using an IRC client please refer to the wiki .","title":"Redbrick InspIRCd"},{"location":"services/irc/#installation","text":"InspIRCd is installed with Nix. There is no Nix package for InspIRCd so we compile a specific git tag from source. See Nix package for details on how it is compiled. Given we only support SSL and require LDAP, we need to enable both at compile time.","title":"Installation"},{"location":"services/irc/#configuration","text":"InspIRCd's configuration is in Nix here . This config will be converted to xml on disc.","title":"Configuration"},{"location":"services/irc/#important-configuration","text":"oper is a list of admin users on the irc server. Their OPER password will need to be manually hashed with hmac-sha256 , and placed in a secret on the server to be read in by inspircd. ldapwhitelist is a list of cidr addresses that do no require authentication. The list consists of Redbrick public and private addresses as well as oldsoc . link is a list of all servers we peer with including the anope services server that runs on the same box.","title":"Important Configuration"},{"location":"services/irc/#oldsocnet","text":"oldsoc.net is a server run by old TCD netsocers. All the users on it are the remaining TCD associates following the shutdown of TCD IRCd. This server is maintained by its own users and has explicit permission to join IRC without LDAP auth.","title":"oldsoc.net"},{"location":"services/irc/#anope","text":"Redbrick runs Anope services for the entire network. As with inspircd we compile from source. Refer to anopes github docs for configuration specifics. Our current Anope is configured with standard mods of chanserv, nickserv and operserv. All config is in here . Anope stores all info in a custom db file on disk.","title":"Anope"},{"location":"services/irc/#discord-bridge-butlerx","text":"We run a bridge between the Redbrick Discord and irc. The configuration for this is here . The bridge adds all users from discord with the suffix _d2 and all irc users appear as them self but tagged as a bot in discord. Not all discord channels are on IRC, the config above contains a mapping of irc channels to discord channels id's. This needs to be manually updated to add more channels.","title":"Discord Bridge - butlerx"},{"location":"services/nfs/","text":"NFS / Network File Storage \u00b6 NFS is used to serve the notorious /storage directory on Icarus to all of Redbrick's machines, which in turn serves /home , /webtree and some other critical folders. Deployment \u00b6 NFS is deployed with Nix on Icarus It is backed onto the PowerVault MD1200 with all its disk passed through single-drive RAID 0s toallow for setup of ZFS: 1 mirror of 2x 500GB drives 1 mirror of 2x 750GB drives 1 mirror of 2x 1TB drives Stripe across all the mirrors for 2TB of usable storage 1 hot spare 750GB drive ZFS is configured with compression onand dedup off The ZFS pool is called zbackup Redbrick Special Notes \u00b6 On each machine where /storage is where NFS is mounted, but /home and /webtree are symlinks into there. There are 2 scripts used to control quotas, detailed below. NFS is backed up to Albus via ZnapZend . zfsquota and zfsquotaquery \u00b6 These are two bash scripts that run as systemd services on Icarus to manage quotas. This is achieved through getting and setting the userquota and userused properties of the ZFS dataset. zfsquota \u00b6 ZFSQuota will read the quota field from LDAP and sync this with the userquota value on the dataset. It is not event driven - it runs on a timer every 3 hours and syncs all LDAP quotas with ZFS. It can be kicked off manually, which is described below. Users with no quota in LDAP will have no quota in /storage , and users who have their quota removed will persist on ZFS. Changing user names has no impact on this since it is synced with uidNumber. zfsquotaquery \u00b6 ZFSQuotaQuery returns the quota and used space of a particular user. This is used to then inform rbquota which provides the data for the MOTD used space report. Both of these scripts are defined and deployed in the Nix config repo. It runs on port 1995/tcp. Operation \u00b6 In general, there isn't too much to do with NFS. Below are some commands of interest for checking its status. # On the NFS server, list the exported filesystems showmount -e # Get the real space usage + fragmentation percent from ZFS zpool list zbackup # Check a user's quota zpool get userquota@m1cr0man zbackup zpool get userused@m1cr0man zbackup # Delete a quota from ZFS (useful if a user is deleted) zpool set userquota@123456 = none zbackup # Get all user quota usage, and sort it by usage zfs userspace -o used,name zbackup | sort -h | tee used_space.txt # Resync quotas (this command will not return until it is finished) systemctl start zfsquota # Check the status of zfsquotaquery systemctl status zfsquotaquery Troubleshooting \u00b6 In the event where clients are unable to read from NFS, your priority should be restoring the NFS server, rather than unmounting NFS from clients. This is because NFS is mounted in hard mode everywhere, meaning that it will block on IO until a request can be fulfilled. Check The Server \u00b6 # Check the ZFS volume is readable and writable ls -l /zbackup/home touch /zbackup/testfile # Check that rpc.mountd, rpc.statd and rpcbind are running and lisening ss -anlp | grep rpc # Check the above services for errors (don't worry about blkmap) systemctl status nfs- { server,idmapd,mountd } journalctl -fu nfs-server -u nfs-idmapd -u nfs-mountd Check The Client \u00b6 # Check for connection to NFS ss -atp | grep nfs # Check the fstab entry grep storage /etc/fstab # Check if the NFS server port can be reached telnet 192 .168.0.150 2049 # Entering gibberish should cause the connection to close # Remount read-only mount -o remount,ro /storage # Not much left you can do but remount entirely or reboot Rolling Back or Restoring a Backup \u00b6 See znapzend","title":"NFS"},{"location":"services/nfs/#nfs-network-file-storage","text":"NFS is used to serve the notorious /storage directory on Icarus to all of Redbrick's machines, which in turn serves /home , /webtree and some other critical folders.","title":"NFS / Network File Storage"},{"location":"services/nfs/#deployment","text":"NFS is deployed with Nix on Icarus It is backed onto the PowerVault MD1200 with all its disk passed through single-drive RAID 0s toallow for setup of ZFS: 1 mirror of 2x 500GB drives 1 mirror of 2x 750GB drives 1 mirror of 2x 1TB drives Stripe across all the mirrors for 2TB of usable storage 1 hot spare 750GB drive ZFS is configured with compression onand dedup off The ZFS pool is called zbackup","title":"Deployment"},{"location":"services/nfs/#redbrick-special-notes","text":"On each machine where /storage is where NFS is mounted, but /home and /webtree are symlinks into there. There are 2 scripts used to control quotas, detailed below. NFS is backed up to Albus via ZnapZend .","title":"Redbrick Special Notes"},{"location":"services/nfs/#zfsquota-and-zfsquotaquery","text":"These are two bash scripts that run as systemd services on Icarus to manage quotas. This is achieved through getting and setting the userquota and userused properties of the ZFS dataset.","title":"zfsquota and zfsquotaquery"},{"location":"services/nfs/#zfsquota","text":"ZFSQuota will read the quota field from LDAP and sync this with the userquota value on the dataset. It is not event driven - it runs on a timer every 3 hours and syncs all LDAP quotas with ZFS. It can be kicked off manually, which is described below. Users with no quota in LDAP will have no quota in /storage , and users who have their quota removed will persist on ZFS. Changing user names has no impact on this since it is synced with uidNumber.","title":"zfsquota"},{"location":"services/nfs/#zfsquotaquery","text":"ZFSQuotaQuery returns the quota and used space of a particular user. This is used to then inform rbquota which provides the data for the MOTD used space report. Both of these scripts are defined and deployed in the Nix config repo. It runs on port 1995/tcp.","title":"zfsquotaquery"},{"location":"services/nfs/#operation","text":"In general, there isn't too much to do with NFS. Below are some commands of interest for checking its status. # On the NFS server, list the exported filesystems showmount -e # Get the real space usage + fragmentation percent from ZFS zpool list zbackup # Check a user's quota zpool get userquota@m1cr0man zbackup zpool get userused@m1cr0man zbackup # Delete a quota from ZFS (useful if a user is deleted) zpool set userquota@123456 = none zbackup # Get all user quota usage, and sort it by usage zfs userspace -o used,name zbackup | sort -h | tee used_space.txt # Resync quotas (this command will not return until it is finished) systemctl start zfsquota # Check the status of zfsquotaquery systemctl status zfsquotaquery","title":"Operation"},{"location":"services/nfs/#troubleshooting","text":"In the event where clients are unable to read from NFS, your priority should be restoring the NFS server, rather than unmounting NFS from clients. This is because NFS is mounted in hard mode everywhere, meaning that it will block on IO until a request can be fulfilled.","title":"Troubleshooting"},{"location":"services/nfs/#check-the-server","text":"# Check the ZFS volume is readable and writable ls -l /zbackup/home touch /zbackup/testfile # Check that rpc.mountd, rpc.statd and rpcbind are running and lisening ss -anlp | grep rpc # Check the above services for errors (don't worry about blkmap) systemctl status nfs- { server,idmapd,mountd } journalctl -fu nfs-server -u nfs-idmapd -u nfs-mountd","title":"Check The Server"},{"location":"services/nfs/#check-the-client","text":"# Check for connection to NFS ss -atp | grep nfs # Check the fstab entry grep storage /etc/fstab # Check if the NFS server port can be reached telnet 192 .168.0.150 2049 # Entering gibberish should cause the connection to close # Remount read-only mount -o remount,ro /storage # Not much left you can do but remount entirely or reboot","title":"Check The Client"},{"location":"services/nfs/#rolling-back-or-restoring-a-backup","text":"See znapzend","title":"Rolling Back or Restoring a Backup"},{"location":"services/servers/","text":"Servers \u00b6 Redbrick provides two main servers (Azazel and Pygmalion) for it's members to use for various use cases, for example running programs. Logging in to Redbrick Servers \u00b6 The main login server used in Redbrick is Azazel. You may also log in to Pygmalion if you wish at pyg.redbrick.dcu.ie You can log in using ssh in your terminal/command prompt application of choice with your Redbrick username and password like so: ssh YOUR_USERNAME@redbrick.dcu.ie # When prompted for the password, please input your Redbrick account password. Forgot your password? Contact an admin on our Discord Server or at elected-admins@redbrick.dcu.ie","title":"Servers"},{"location":"services/servers/#servers","text":"Redbrick provides two main servers (Azazel and Pygmalion) for it's members to use for various use cases, for example running programs.","title":"Servers"},{"location":"services/servers/#logging-in-to-redbrick-servers","text":"The main login server used in Redbrick is Azazel. You may also log in to Pygmalion if you wish at pyg.redbrick.dcu.ie You can log in using ssh in your terminal/command prompt application of choice with your Redbrick username and password like so: ssh YOUR_USERNAME@redbrick.dcu.ie # When prompted for the password, please input your Redbrick account password. Forgot your password? Contact an admin on our Discord Server or at elected-admins@redbrick.dcu.ie","title":"Logging in to Redbrick Servers"},{"location":"services/znapzend/","text":"ZnapZend \u00b6 Overview \u00b6 ZnapZend is used to back up the NFS ZFS dataset from our NFS server to Albus. It can also be used to back up other ZFS datasets on other hosts, but at the time of writing NFS is the only thing being backed up this way. ZnapZend runs on the client and sends backups to Albus over SSH using zfs send | zfs receive piping. The backup strategy can be viewed in the NixOS configuration . Adding Another Backup \u00b6 There is not much manual configuration to add a host to the ZnapZend backups. Create an SSH key for the root user with no passphrase on the host you want to send the backups from. Use ssh-keygen -t ed25519 . Add this new SSH public key to the rbbackup user's authorized keys on Albus . Try SSHing to rbbackups@albus.internal to load the host key and test the passwordless authentication. Import the znapzend service config on the sending host and configure redbrick.znapzendSourceDataset and redbrick.znapzendDestDataset . Then apply the config. NOTE The DestDataset must be unique across all configured backups/servers. Debugging \u00b6 Znapzend runs at the top of every hour to make backups. You can watch the progress with journalctl -fu znapzend.service . Failures are usually caused by incorrect SSH configuration, so make sure that passwordless auth using the sending host's root SSH key is working. Rolling Back NFS \u00b6 If the NFS server is online and functional, you do not need to involve Albus to roll back changes, as all the snapshots are kept on Icarus too. Find the snapshot you want to restore with zfs list -t snapshot. Run zfs rollback $snapshotname. That's it! These instructions obviously work for backups other than NFS too, should any ever exist. Restoring NFS from a backup \u00b6 If the NFS server has died or you are creating a copy of it, here's how to pull the dataset from Albus, On Albus, find the snapshot you want to restore with zfs list -t snapshot . Open a screen/tmux, and copy the snapshot to a dataset in your target ZFS pool with ssh albus zfs send -vRLec $snapshotname | zfs receive $newpool/$datasetname .","title":"ZnapZend"},{"location":"services/znapzend/#znapzend","text":"","title":"ZnapZend"},{"location":"services/znapzend/#overview","text":"ZnapZend is used to back up the NFS ZFS dataset from our NFS server to Albus. It can also be used to back up other ZFS datasets on other hosts, but at the time of writing NFS is the only thing being backed up this way. ZnapZend runs on the client and sends backups to Albus over SSH using zfs send | zfs receive piping. The backup strategy can be viewed in the NixOS configuration .","title":"Overview"},{"location":"services/znapzend/#adding-another-backup","text":"There is not much manual configuration to add a host to the ZnapZend backups. Create an SSH key for the root user with no passphrase on the host you want to send the backups from. Use ssh-keygen -t ed25519 . Add this new SSH public key to the rbbackup user's authorized keys on Albus . Try SSHing to rbbackups@albus.internal to load the host key and test the passwordless authentication. Import the znapzend service config on the sending host and configure redbrick.znapzendSourceDataset and redbrick.znapzendDestDataset . Then apply the config. NOTE The DestDataset must be unique across all configured backups/servers.","title":"Adding Another Backup"},{"location":"services/znapzend/#debugging","text":"Znapzend runs at the top of every hour to make backups. You can watch the progress with journalctl -fu znapzend.service . Failures are usually caused by incorrect SSH configuration, so make sure that passwordless auth using the sending host's root SSH key is working.","title":"Debugging"},{"location":"services/znapzend/#rolling-back-nfs","text":"If the NFS server is online and functional, you do not need to involve Albus to roll back changes, as all the snapshots are kept on Icarus too. Find the snapshot you want to restore with zfs list -t snapshot. Run zfs rollback $snapshotname. That's it! These instructions obviously work for backups other than NFS too, should any ever exist.","title":"Rolling Back NFS"},{"location":"services/znapzend/#restoring-nfs-from-a-backup","text":"If the NFS server has died or you are creating a copy of it, here's how to pull the dataset from Albus, On Albus, find the snapshot you want to restore with zfs list -t snapshot . Open a screen/tmux, and copy the snapshot to a dataset in your target ZFS pool with ssh albus zfs send -vRLec $snapshotname | zfs receive $newpool/$datasetname .","title":"Restoring NFS from a backup"}]}